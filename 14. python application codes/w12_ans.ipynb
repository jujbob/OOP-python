{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ece5ba8b14a4a3facdec5a6761ddd13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/651 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7427d72df64147e5a72181bcd9c29327",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/251M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7188b61b438f4aa0bef423460fdb1a05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2290532b5a2454597917edb6e155adc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/685 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afcb353caa04484f93c283c112a0f86e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f45835df6ae945f888f8730c190f01cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1ac0b8da9c04d8eb90f7e636649035d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/441 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = transformers.GPT2Tokenizer.from_pretrained(\n",
    "    \"facebook/opt-125m\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. \n",
    "\n",
    "자연어처리에서는 컴퓨터가 텍스트를 인식하기 위해, 텍스트를 토큰 단위로 쪼개고 이 토큰을 자연수로 매핑합니다. 이 과정을 encoding이라고 하며, 다음의 과정으로 진행됩니다:\n",
    "\n",
    "`string --(tokenize)--> tokens --(mapping to natural numbers)--> input_ids`\n",
    "\n",
    "transformers의 tokenizer document를 읽어보며 아래의 문제를 풀어보세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'Ġam', 'Ġso', 'Ġhappy', '.', 'ĠHow', 'Ġare', 'Ġyou', 'ĠKy', 'ung', 't', 'ae', 'ĠLim', '?']\n"
     ]
    }
   ],
   "source": [
    "## 1.1. string을 토큰으로 변환하는 함수를 찾아보세요. (tokenize)\n",
    "tokens = tokenizer.tokenize(\"I am so happy. How are you Kyungtae Lim?\")\n",
    "print(tokens)\n",
    "\n",
    "# output: ['I', 'Ġam', 'Ġso', 'Ġhappy', '.', 'ĠHow', 'Ġare', 'Ġyou', 'ĠKy', 'ung', 't', 'ae', 'ĠLim', '?']\n",
    "# 여기에서 Ġ는 단어 앞의 공백을 나타내는 특수 토큰."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 100, 524, 98, 1372, 4, 1336, 32, 47, 5959, 1545, 90, 4791, 8300, 116]\n"
     ]
    }
   ],
   "source": [
    "## 1.2. string을 토큰화한 뒤, 자연수로 매핑하는 함수를 찾아보세요. (encoding)\n",
    "input_ids = tokenizer.encode(\"I am so happy. How are you Kyungtae Lim?\")\n",
    "print(input_ids)\n",
    "\n",
    "# output: [2, 100, 524, 98, 1372, 4, 1336, 32, 47, 5959, 1545, 90, 4791, 8300, 116]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   2,  100,  524,   98, 1372,    4, 1336,   32,   47, 5959, 1545,   90,\n",
      "         4791, 8300,  116]])\n"
     ]
    }
   ],
   "source": [
    "## 1.3. encoding하는 함수에 인자를 추가하여 pytorch의 tensor로 return하는 코드를 작성하세요.\n",
    "input_ids_tensor = tokenizer.encode(\"I am so happy. How are you Kyungtae Lim?\", return_tensors=\"pt\")\n",
    "print(input_ids_tensor)\n",
    "\n",
    "# output: tensor([[   2,  100,  524,   98, 1372,    4, 1336,   32,   47, 5959, 1545,   90, 4791, 8300,  116]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am so happy. How are you Kyungtae Lim?\n"
     ]
    }
   ],
   "source": [
    "## 1.4. 자연수로 매핑된 토큰을 다시 string으로 복원하는 함수를 찾아보세요.\n",
    "input_ids = tokenizer.encode(\"I am so happy. How are you Kyungtae Lim?\") # encoding\n",
    "\n",
    "decoded_text = tokenizer.decode(input_ids, skip_special_tokens=True) # decoding\n",
    "print(decoded_text)\n",
    "\n",
    "# output: I am so happy. How are you Kyungtae Lim?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.\n",
    "\n",
    "`tokenizer.get_vocab()`을 사용하면, dictionary 형태로 tokenizer의 vocabulary를 얻을 수 있습니다.\n",
    "\n",
    "이것을 활용하여 `input_ids`를 다시 string으로 변환하는 함수를 작성해보세요.\n",
    "\n",
    "(직접 구현)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s>I am so happy. How are you Kyungtae Lim?\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(\"I am so happy. How are you Kyungtae Lim?\")\n",
    "\n",
    "vocab = tokenizer.get_vocab()\n",
    "inv_vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "decoded_text = \"\"\n",
    "\n",
    "for ids in input_ids:\n",
    "    decoded_text += inv_vocab[ids]\n",
    "\n",
    "decoded_text = decoded_text.replace(\"Ġ\", \" \")\n",
    "print(decoded_text)\n",
    "\n",
    "# output: </s>I am so happy. How are you Kyungtae Lim?\n",
    "# 여기에서, </s>는 텍스트의 시작을 나타내는 특수토큰으로, encoding 과정에서 자동으로 생성됨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.\n",
    "\n",
    "tokenizer 에서는 list로 구성된 여러 문장을 한 번에 토큰화 수 있는 `batch_encode_plus` 함수가 제공됩니다.\n",
    "\n",
    "document의 `batch_encode_plus` 함수를 살펴보고, 동일한 출력값을 내도록 코드를 완성해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[2, 100, 524, 14791, 879, 1090, 1638, 4, 38, 437, 19222, 4, 38, 524, 5, 8260, 313, 11, 5, 232, 4], [2, 7939, 18, 1032, 6, 14457, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "text = [\n",
    "    \"I am hyunseok. I'm handsome. I am the strongest man in the world.\",\n",
    "    \"Let's fight, Tyson.\"\n",
    "]\n",
    "\n",
    "inputs = tokenizer.batch_encode_plus(text, padding=True, return_attention_mask=False)\n",
    "print(inputs)\n",
    "# output: {'input_ids': [[2, 100, 524, 14791, 879, 1090, 1638, 4, 38, 437, 19222, 4, 38, 524, 5, 8260, 313, 11, 5, 232, 4], [2, 7939, 18, 1032, 6, 14457, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I am hyunseok. I'm handsome. I am the strongest man in the world.\",\n",
       " \"Let's fight, Tyson.\"]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## batch_encode_plus와 유사하게, 여러 문장을 한 번에 decoding 할 수 있는 함수를 찾아보세요.\n",
    "\n",
    "tokenizer.batch_decode(inputs['input_ids'], skip_special_tokens=True)\n",
    "\n",
    "# output: \n",
    "# [\"I am hyunseok. I'm handsome. I am the strongest man in the world.\",\n",
    "#  \"Let's fight, Tyson.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 모델 추론\n",
    "\n",
    "언어모델에게 질문을 하고 답변을 받아보세요! \n",
    "\n",
    "이 모델은 크기가 매우 작아서 생각보다 멍청합니다. \n",
    "\n",
    "https://huggingface.co/docs/transformers/v4.46.3/en/main_classes/text_generation#transformers.GenerationConfig\n",
    "\n",
    "\n",
    "위 링크의 Generation config를 살펴보며, 여러 인자를 조절하며 답변을 받아보세요.\n",
    "\n",
    "\n",
    "힌트: 'Parameters for manipulation of the model output logits' 섹션"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.OPTForCausalLM.from_pretrained(\n",
    "    \"facebook/opt-125m\",\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  Who is jisung park?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nJisung Park in Nakhon Rung. A Vietnamese resort at South Gate, it boasts a few shops and eateries.</s>'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(\"Who is jisung park?\", return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "output = model.generate(\n",
    "    inputs=input_ids,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=True,\n",
    "    # num_beams=3,\n",
    "    # temperature=1.2,\n",
    "    # repetition_penalty=1.4,\n",
    ")\n",
    "\n",
    "print(\"input: \", tokenizer.decode(input_ids[0], skip_special_tokens=True))\n",
    "tokenizer.decode(output[0][len(input_ids[0]):])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".score_llama_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
